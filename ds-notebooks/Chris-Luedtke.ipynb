{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull Data from USGS\n",
    "\n",
    "https://earthquake.usgs.gov/fdsnws/event/1/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import folium\n",
    "from folium import plugins\n",
    "from io import StringIO\n",
    "import requests\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "\n",
    "payload = {\n",
    "    'format': 'csv', \n",
    "#     'starttime': None,  # default last 30 days\n",
    "#     'endtime': '2019-06-03',  # default now\n",
    "    'minmagnitude': 2.5,  # default null\n",
    "    'limit': None,  # default null, returns 404 over 20,000\n",
    "}\n",
    "url = 'https://earthquake.usgs.gov/fdsnws/event/1/query'\n",
    "r = requests.get(url, params=payload)\n",
    "\n",
    "df = pd.read_csv(StringIO(r.text))\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Animated Map with `folium`\n",
    "\n",
    "https://nbviewer.jupyter.org/github/python-visualization/folium/blob/master/examples/Plugins.ipynb#Timestamped-GeoJSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get faults\n",
    "r = requests.get('https://raw.githubusercontent.com/'\n",
    "                 'fraxen/tectonicplates/master/GeoJSON/'\n",
    "                 'PB2002_boundaries.json')\n",
    "\n",
    "fault_features = r.json()['features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    {\n",
    "        'type': 'Feature',\n",
    "        'geometry': {\n",
    "            'type': 'Point',\n",
    "            'coordinates': [r['longitude'], r['latitude']],\n",
    "        },\n",
    "        'properties': {\n",
    "            'time': r['time'][0:-1],\n",
    "            'popup': (\n",
    "                f\"<strong>Time:</strong> {r['time']}<br>\"\n",
    "                f\"<strong>Place:</strong> {r['place']}<br>\"\n",
    "                f\"<strong>Magnitude:</strong> {r['mag']} {r['magType']}<br>\"\n",
    "                f\"<strong>Depth:</strong> {r['depth']}<br>\"\n",
    "            ),\n",
    "            'icon': 'circle',\n",
    "            'iconstyle': {\n",
    "                'fillOpacity': 0.5,\n",
    "                'stroke': 0,\n",
    "                'radius': r['mag'] * 2.5\n",
    "            },\n",
    "        }\n",
    "    } for i, r in df.iterrows()\n",
    "]\n",
    "\n",
    "m = folium.Map(\n",
    "#     location=()\n",
    "    tiles='CartoDBpositron',\n",
    "#     zoom_start=1,\n",
    "#     no_wrap=True,\n",
    "    min_zoom=1.5,\n",
    "    max_zoom=5,\n",
    "    world_copy_jump=True,\n",
    ")\n",
    "\n",
    "# add faults\n",
    "folium.GeoJson(\n",
    "    {\n",
    "        'type': 'FeatureCollection',\n",
    "        'features': fault_features,\n",
    "    },\n",
    "    style_function = lambda x: {\n",
    "        'color': 'red',\n",
    "        'weight': 0.5,\n",
    "    }\n",
    ").add_to(m)\n",
    "\n",
    "plugins.TimestampedGeoJson(\n",
    "    {\n",
    "        'type': 'FeatureCollection',\n",
    "        'features': features\n",
    "    },\n",
    "    period='PT6H', # six hour\n",
    "    time_slider_drag_update=True,\n",
    "    duration='PT12H',\n",
    "    date_options='YYYY-MM-DD HH UTC'\n",
    ").add_to(m)\n",
    "\n",
    "folium.plugins.Fullscreen(\n",
    "    position='topright',\n",
    "    force_separate_button=True,\n",
    ").add_to(m)\n",
    "\n",
    "# m.save('earthquakes.html')\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Data\n",
    "\n",
    "#### Decreasing Reported Events in Previous 30 Days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "    'format': 'csv', \n",
    "    'starttime': '2019-01-01',  # default last 30 days\n",
    "    'minmagnitude': 4,  # default null\n",
    "    'limit': None,  # default null, returns 404 over 20,000\n",
    "}\n",
    "url = 'https://earthquake.usgs.gov/fdsnws/event/1/query'\n",
    "r = requests.get(url, params=payload)\n",
    "\n",
    "df = pd.read_csv(StringIO(r.text))\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "df['updated'] = pd.to_datetime(df['updated'])\n",
    "\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['week_of'] = (df['time'].dt.floor('D') - pd.to_timedelta(df['time'].dt.dayofweek, unit='d'))\n",
    "df['mag_rnd'] = df['mag'].apply(np.floor) # floor round to nearest 0.5 mag\n",
    "\n",
    "week_mag_counts = (df.loc[(df['week_of'].min() < df['week_of']) & \n",
    "                          (df['week_of'] < df['week_of'].max()) &\n",
    "                          (df['mag_rnd'] <= 6)]\n",
    "                     .groupby(['mag_rnd', 'week_of'])\n",
    "                     .size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "week_mag_counts.unstack(level=0).plot(ax=ax)\n",
    "\n",
    "ax.legend(title='Magnitude')\n",
    "ax.set_xlabel('Date (events grouped by week)')\n",
    "ax.set_ylabel('Number of Events')\n",
    "\n",
    "ax.set_ybound(lower=0)\n",
    "ax.set_facecolor('#D4DADC')\n",
    "\n",
    "for item in [ax.xaxis.label, ax.yaxis.label]:   \n",
    "    item.set_fontweight('bold')\n",
    "\n",
    "fig.savefig(\n",
    "    '../ds-application/app/static/recency_freq.png', \n",
    "    facecolor=fig.get_facecolor(),\n",
    "    dpi=160,\n",
    "    transparent=False, \n",
    "    bbox_inches='tight',\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['days_to_update'] = (df['updated'] - df['time']).dt.days\n",
    "\n",
    "df_to_hist = pd.DataFrame({k: v.reset_index(drop=True) for k, v in df.loc[df['time'] > '2018'].groupby('mag_rnd')['days_to_update']})\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "df_to_hist.plot.hist(stacked=True, ax=ax, bins=16)\n",
    "\n",
    "ax.legend(title='Magnitude')\n",
    "ax.set_xlabel('Age of Record when Updated (Days)')\n",
    "ax.set_ylabel('Number of Records')\n",
    "\n",
    "ax.set_ybound(lower=0)\n",
    "# fig.set_facecolor('#FAFAF8')\n",
    "ax.set_facecolor('#D4DADC')\n",
    "\n",
    "for item in [ax.xaxis.label, ax.yaxis.label]:\n",
    "#     item.set_color('#8B99A4')\n",
    "    item.set_fontweight('bold')\n",
    "\n",
    "fig.savefig(\n",
    "    '../ds-application/app/static/update_dist.png', \n",
    "    facecolor=fig.get_facecolor(),\n",
    "    dpi=160,\n",
    "    transparent=False,\n",
    "    bbox_inches='tight',\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download USGS Data 1989-2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dateutil.parser import parse\n",
    "import requests\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "def dl_quake_data(start_date, end_date, page_limt=10000):\n",
    "    start_date = parse(start_date).isoformat()\n",
    "    end_date = parse(end_date).isoformat()\n",
    "    payload = {\n",
    "        'format': 'csv',\n",
    "        'starttime': start_date,\n",
    "        'endtime': end_date,\n",
    "        'minmagnitude': 2,\n",
    "        'limit': page_limt,\n",
    "        'orderby': 'time-asc',\n",
    "    }\n",
    "    url = 'https://earthquake.usgs.gov/fdsnws/event/1/query'\n",
    "    r = requests.get(url, params=payload)\n",
    "    \n",
    "    if r.status_code != 200:\n",
    "        print('Error', r.status_code, r.url)\n",
    "        return False\n",
    "    \n",
    "    df = pd.read_csv(StringIO(r.text))\n",
    "    \n",
    "    dt_min = df['time'].iloc[0]\n",
    "    dt_max = df['time'].iloc[-1]\n",
    "    \n",
    "    fn = (f'{parse(dt_min).strftime(\"%Y-%m-%d\")}_'\n",
    "          f'{parse(dt_max).strftime(\"%Y-%m-%d\")}')\n",
    "    df.to_csv(f'data/{fn}.csv', index=False)\n",
    "    \n",
    "    print(fn)\n",
    "    \n",
    "    if len(df) == page_limt:\n",
    "         dl_quake_data(start_date=dt_max,\n",
    "                       end_date=end_date)\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# done '1989-01-01' to '2019-01-01'\n",
    "# dl_quake_data('1990-11-15', '1999-01-01', 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "dfs = []\n",
    "for csv in Path('data').iterdir():\n",
    "    dfs.append(pd.read_csv(csv))\n",
    "    \n",
    "df = pd.concat(dfs)\n",
    "df = df.drop_duplicates(['id'])\n",
    "\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "df['updated'] = pd.to_datetime(df['updated'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "def OLS_mc_a_b(df, remove_tail=True):\n",
    "    df = df.copy()\n",
    "    df['mag'] = df['mag'].round(1)\n",
    "    \n",
    "    if remove_tail:\n",
    "        df = df.loc[df['mag']<6]\n",
    "    \n",
    "    n_steps = int((df['mag'].max() - df['mag'].min()) * 10) + 1\n",
    "    mag_range = np.linspace(df['mag'].min(), df['mag'].max(), n_steps)\n",
    "    mag_range = [round(x, 2) for x in mag_range]\n",
    "    \n",
    "    counts = pd.Series(  # cumulative sum\n",
    "        index=mag_range,\n",
    "        data=[sum(df['mag'] >= x) for x in mag_range]\n",
    "    )\n",
    "\n",
    "    best_score = 0\n",
    "    best_m_c, a, b = [None] * 3\n",
    "\n",
    "    for M_c in np.arange(df['mag'].min(), 4.6, 0.1):\n",
    "        M_c = round(M_c, 2)\n",
    "        data = counts.loc[counts.index>=M_c]\n",
    "        X = np.array(data.index).reshape(-1, 1)\n",
    "        y = np.log10(data.values)\n",
    "        reg = LinearRegression().fit(X, y)\n",
    "        score = reg.score(X, y)\n",
    "\n",
    "        if score > best_score:\n",
    "            best_m_c = M_c\n",
    "            best_score = score\n",
    "            a = reg.intercept_\n",
    "            b = -1 * reg.coef_[0]\n",
    "\n",
    "    return best_m_c, best_score, a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.copy()\n",
    "df['mag'] = df['mag'].round(1)\n",
    "\n",
    "n_steps = int((df['mag'].max() - df['mag'].min()) * 10) + 1\n",
    "mag_range = np.linspace(df['mag'].min(), df['mag'].max(), n_steps)\n",
    "mag_range = [round(x, 2) for x in mag_range]\n",
    "\n",
    "counts = pd.Series(  # cumulative sum\n",
    "    index=mag_range,\n",
    "    data=[sum(df['mag'] >= x) for x in mag_range]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_m_c, best_score, a, b = OLS_mc_a_b(df, remove_tail=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "\n",
    "np.log10(counts).plot(ax=ax, label='Observed Events')\n",
    "ax.plot([2, 9], [(a - b*x) for x in [2, 9]], label='Expected Events')\n",
    "ax.vlines(best_m_c, 0, 9, linestyle='--', label='Magnitude Threshold')\n",
    "\n",
    "ax.legend()\n",
    "ax.set_xlabel('Magnitude')\n",
    "ax.set_ylabel('Number of Events  â‰¥  Magnitude')\n",
    "\n",
    "ax.set_ybound(lower=0, upper=8.5)\n",
    "ax.set_facecolor('#D4DADC')\n",
    "ax.set_yticklabels([0] + [f\"$10^{x}$\" for x in range(1, 10)])\n",
    "\n",
    "for item in [ax.xaxis.label, ax.yaxis.label]:\n",
    "#     item.set_color('#8B99A4')\n",
    "    item.set_fontweight('bold')\n",
    "\n",
    "fig.savefig(\n",
    "    '../ds-application/app/static/gr_law.png', \n",
    "    facecolor=fig.get_facecolor(),\n",
    "    dpi=160,\n",
    "    transparent=False,\n",
    "    bbox_inches='tight',\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group Earthquake Events to Region\n",
    "\n",
    "This is done because different regions have different detective power for small magnitude eathquakes. Here I generate a grid of equi-distant points around the globe before clustering earthquake events to the nearest node.\n",
    "\n",
    "As improvements, I could define these regions algorithmically (like with [HDBSCAN](https://hdbscan.readthedocs.io/en/latest/index.html), [clusterpy](https://github.com/clusterpy/), [Moran'sI](https://en.wikipedia.org/wiki/Moran's_I)) based on earthquake characteristics in each regions. It may be that a special model already exists for this purpose [in extant research](https://www.researchgate.net/publication/260702383_A_detailed_seismic_zonation_model_for_shallow_earthquakes_in_the_broader_Aegean_area). Alternatively, knowing where seismic stations are located and how sensitive they are could help inform region definitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import hdbscan\n",
    "\n",
    "# downsamp = df[['latitude', 'longitude']].sample(frac=0.05)\n",
    "# clusterer = hdbscan.HDBSCAN(metric='haversine')\n",
    "# clusterer.fit(downsamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_equidist_pts(num_pts=4000):\n",
    "    indices = np.arange(0, num_pts, dtype=float) + 0.5\n",
    "\n",
    "    phi = np.arccos(1 - 2*indices/num_pts)\n",
    "    theta = np.pi * (1 + 5**0.5) * indices\n",
    "\n",
    "    x = np.cos(theta) * np.sin(phi)\n",
    "    y = np.sin(theta) * np.sin(phi)\n",
    "    z = np.cos(phi)\n",
    "\n",
    "    df = pd.DataFrame({'id':range(num_pts),\n",
    "                       'latitude':  180 * np.arcsin(z / 1) / np.pi, \n",
    "                       'longitude': 180 * np.arctan2(y, x) / np.pi})\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = folium.Map(\n",
    "    tiles='CartoDBpositron',\n",
    "    world_copy_jump=False,\n",
    "    zoom_start=1,\n",
    "    no_wrap=True,\n",
    ")\n",
    "\n",
    "for i, r in gen_equidist_pts().iterrows():\n",
    "    folium.CircleMarker(\n",
    "        (r['latitude'], r['longitude']),\n",
    "        stroke=False,\n",
    "        radius=2,\n",
    "        fill=True,\n",
    "        fill_opacity=0.75\n",
    "    ).add_to(m)\n",
    "\n",
    "plugins.Fullscreen(\n",
    "    position='topright', \n",
    "    force_separate_button=True\n",
    ").add_to(m)\n",
    "\n",
    "m.save('../ds-application/app/static/eq_pts.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign Quake Events to Cluster Centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import DistanceMetric\n",
    "\n",
    "def nearest_left_merge(left_df, right_df, latlon_cols=['latitude', 'longitude'], step=50000):\n",
    "    \"\"\"For all rows of the left dataframe, merge in the nearest row from the right\n",
    "    dataframe.\n",
    "    \n",
    "    left: 2D array of [lat, long]\n",
    "    right: 2D array of [lat, long]\n",
    "    \"\"\"\n",
    "    left_df = left_df.copy()\n",
    "    right_df = right_df.copy()\n",
    "    right_df = right_df.reset_index(drop=True)\n",
    "    \n",
    "    dist = DistanceMetric.get_metric('haversine') # requires radians\n",
    "    left_rads = left_df[latlon_cols].values * np.pi / 180\n",
    "    rght_rads = right_df[latlon_cols].values * np.pi / 180\n",
    "\n",
    "    nearest_centers = []\n",
    "\n",
    "    i = 0\n",
    "    while i < len(left_df):\n",
    "        dists = dist.pairwise(left_rads[i:i+step], rght_rads)\n",
    "        indices = np.argmin(dists, axis=1)  # indices of minimum distance node\n",
    "        dists = dists[np.arange(len(dists)), indices].reshape(-1, 1)\n",
    "        result = np.hstack([right_df.values[indices], dists])\n",
    "        nearest_centers.append(result)\n",
    "        i += step\n",
    "    \n",
    "    df = pd.DataFrame(np.vstack(nearest_centers), \n",
    "                      columns=list(right_df) + ['dist'])\n",
    "    df['dist'] = df['dist'] * 6371  # to km\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clust_df = gen_equidist_pts(num_pts=4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To filter down the number of regions, first determine which\n",
    "# regions have ever had a 5.5+ quake\n",
    "nearest_nodes = nearest_left_merge(df.loc[df['mag'] >= 5.5], clust_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_dist = nearest_nodes['dist'].max()\n",
    "print(max_dist)\n",
    "nearest_nodes['dist'].hist(bins=50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearest_nodes.groupby(['id']).size().hist(bins=30);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# filter to regions that have ever had a 5.5+ quake\n",
    "clust_df = clust_df.loc[clust_df['id'].isin(nearest_nodes['id'])]\n",
    "print(clust_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "881/4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = folium.Map(\n",
    "    tiles='CartoDBpositron',\n",
    "    world_copy_jump=False,\n",
    "    zoom_start=1,\n",
    "    no_wrap=True,\n",
    ")\n",
    "\n",
    "for i, r in clust_df.iterrows():\n",
    "    folium.CircleMarker(\n",
    "        (r['latitude'], r['longitude']),\n",
    "        stroke=False,\n",
    "        radius=2,\n",
    "        fill=True,\n",
    "        fill_opacity=0.75\n",
    "    ).add_to(m)\n",
    "\n",
    "plugins.Fullscreen(\n",
    "    position='topright', \n",
    "    force_separate_button=True\n",
    ").add_to(m)\n",
    "\n",
    "m.save('../ds-application/app/static/eq_pts_filtered.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute all events\n",
    "all_nearest_nodes = nearest_left_merge(df, clust_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join\n",
    "df = df.reset_index(drop=True)\n",
    "df = df.join(all_nearest_nodes, rsuffix='_clust')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.groupby('id_clust').size().hist(bins=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[df['dist'] <= max_dist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['dist'].hist(bins=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weeks spanned by dataset\n",
    "n_weeks = len(pd.date_range(df['time'].min().round('D'),\n",
    "                            df['time'].max().round('D'), \n",
    "                            freq='W-SUN'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n weeks in which a mag 5.5+ occured, by node\n",
    "n_weeks_mag55 = (df.loc[df['mag'] >= 5.5]\n",
    "                   .groupby(['id_clust', \n",
    "                             pd.Grouper(key='time', freq='W-SUN')])\n",
    "                   .size()\n",
    "                   .reset_index()\n",
    "                   .groupby('id_clust')\n",
    "                   .size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mag55_liklihoods = ((n_weeks_mag55 / n_weeks) * 100).round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mag55_liklihoods.hist();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mag55_liklihoods.name = '%_liklihood'\n",
    "clust_df = clust_df.merge(mag55_liklihoods.reset_index(), left_on='id', right_on='id_clust')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add zeros back in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "og_clust_df = gen_equidist_pts(num_pts=4000)\n",
    "og_clust_df = og_clust_df.loc[~og_clust_df['id'].isin(clust_df['id'])]\n",
    "og_clust_df['%_liklihood'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clust_df = clust_df.append(og_clust_df, sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assess m_c for each node\n",
    "results = []\n",
    "for n_id, n_df in df.groupby('id_clust'):\n",
    "    best_m_c, best_score, a, b = OLS_mc_a_b(n_df, remove_tail=False)\n",
    "    results.append({\n",
    "        'id_clust': n_id,\n",
    "        'best_m_c': best_m_c,\n",
    "        'best_score': best_score,\n",
    "        'a': a,\n",
    "        'b': b,\n",
    "    })\n",
    "    \n",
    "results = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['best_m_c'].hist();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['best_score'].hist(bins=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_gradient(start_hex, finish_hex=\"#FFFFFF\", n=10, return_hex=True):\n",
    "    '''Generate a gradient list of (n) colors\n",
    "\n",
    "    start_hex, finish_hex: full six-digit color string (\"#FFFFFF\")\n",
    "    '''\n",
    "\n",
    "    s = hex_to_RGB(start_hex)\n",
    "    f = hex_to_RGB(finish_hex)\n",
    "\n",
    "    RGB_list = [s]\n",
    "    # Calcuate a color at each evenly spaced value of t from 1 to n\n",
    "    for t in range(1, n):\n",
    "        # Interpolate RGB vector for color at the current value of t\n",
    "        curr_vector = []\n",
    "        for j in range(3):\n",
    "            rgb_part = int(s[j] + (float(t) / (n-1)) * (f[j] - s[j]))\n",
    "            curr_vector.append(rgb_part)\n",
    "\n",
    "        # Add it to our list of output colors\n",
    "        RGB_list.append(curr_vector)\n",
    "\n",
    "    if return_hex:\n",
    "        RGB_list = [RGB_to_hex(color) for color in RGB_list]\n",
    "\n",
    "    return RGB_list\n",
    "\n",
    "def RGB_to_hex(RGB):\n",
    "    \"\"\"[255,255,255] -> '#FFFFFF'\"\"\"\n",
    "    # Components need to be integers for hex to make sense\n",
    "    RGB = [int(x) for x in RGB]\n",
    "    return \"#\"+\"\".join([\"0{0:x}\".format(v) if v < 16 else\n",
    "                        \"{0:x}\".format(v) for v in RGB])\n",
    "\n",
    "def hex_to_RGB(hex):\n",
    "    \"\"\"'#FFFFFF' -> [255,255,255]\"\"\"\n",
    "    # Pass 16 to the integer function for change of base\n",
    "    return [int(hex[i:i+2], 16) for i in range(1,6,2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_levels = 10\n",
    "colors = linear_gradient('#f3bd90', '#d7191c', n_levels)\n",
    "results['color'] = pd.cut(results['best_score'],\n",
    "                          bins=n_levels, labels=colors)\n",
    "\n",
    "m = folium.Map(\n",
    "    tiles='CartoDBpositron',\n",
    "    world_copy_jump=False,\n",
    "    zoom_start=1,\n",
    "    no_wrap=True,\n",
    ")\n",
    "\n",
    "for i, r in clust_df.merge(results, on='id_clust').iterrows():\n",
    "    folium.CircleMarker(\n",
    "        (r['latitude'], r['longitude']),\n",
    "        stroke=False,\n",
    "        radius=2,\n",
    "        fill=True,\n",
    "        fill_color=r['color_y'],\n",
    "        fill_opacity=0.75\n",
    "    ).add_to(m)\n",
    "\n",
    "plugins.Fullscreen(\n",
    "    position='topright', \n",
    "    force_separate_button=True\n",
    ").add_to(m)\n",
    "\n",
    "# m.save('../ds-application/app/static/eq_pts_filtered.html')\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Colored Circle Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_levels = 10\n",
    "colors = linear_gradient('#f3bd90', '#d7191c', n_levels)\n",
    "clust_df['color'] = pd.cut(clust_df['%_liklihood'],\n",
    "                           bins=n_levels, labels=colors)\n",
    "\n",
    "m = folium.Map(\n",
    "    tiles='CartoDBpositron',\n",
    "    world_copy_jump=True,\n",
    ")\n",
    "\n",
    "# add faults\n",
    "for i, r in clust_df.loc[clust_df['%_liklihood'] > 0].iterrows():\n",
    "#     if r['%_liklihood'] == 0:\n",
    "#         continue\n",
    "    folium.CircleMarker(\n",
    "        (r['latitude'], r['longitude']),\n",
    "        radius=4,\n",
    "        stroke=False,\n",
    "        fill=True,\n",
    "        fill_color=r['color'],\n",
    "        fill_opacity=0.75,\n",
    "        tooltip=str(r['%_liklihood']),\n",
    "    ).add_to(m)\n",
    "\n",
    "m.save('test.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Contour Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# adapted from https://www.tjansson.dk/2018/10/contour-map-in-folium/\n",
    "import json\n",
    "import branca\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import griddata\n",
    "import geojsoncontour\n",
    "import scipy as sp\n",
    "import scipy.ndimage\n",
    " \n",
    "# Setup colormap\n",
    "colors = linear_gradient('#f3bd90', '#d7191c', 15)\n",
    "vmin   = 0\n",
    "vmax   = 15\n",
    "levels = len(colors)\n",
    "cm     = branca.colormap.LinearColormap(colors, vmin=vmin, vmax=vmax).to_step(levels)\n",
    " \n",
    "# The original data\n",
    "x_orig = np.asarray(clust_df['longitude'].tolist())\n",
    "y_orig = np.asarray(clust_df['latitude'].tolist())\n",
    "z_orig = np.asarray(clust_df['%_liklihood'].tolist())\n",
    " \n",
    "# Make a grid\n",
    "x_arr          = np.linspace(-180, 180, 1000)\n",
    "y_arr          = np.linspace( -90,  90, 1000)\n",
    "x_mesh, y_mesh = np.meshgrid(x_arr, y_arr)\n",
    " \n",
    "# Grid the values\n",
    "z_mesh = griddata((x_orig, y_orig), z_orig, (x_mesh, y_mesh), method='linear')\n",
    " \n",
    "# Gaussian filter the grid to make it smoother\n",
    "sigma = [3, 3]\n",
    "z_mesh = sp.ndimage.filters.gaussian_filter(z_mesh, sigma, mode='constant')\n",
    "\n",
    "# Create the contour\n",
    "contourf = plt.contourf(x_mesh, y_mesh, z_mesh, levels, alpha=0.5, colors=colors, \n",
    "                        linestyles='None', vmin=vmin, vmax=vmax);\n",
    " \n",
    "# Convert matplotlib contourf to geojson\n",
    "geojson = geojsoncontour.contourf_to_geojson(\n",
    "    contourf=contourf,\n",
    "    min_angle_deg=3.0,\n",
    "    ndigits=5,\n",
    "    stroke_width=1,\n",
    "    fill_opacity=0.5,\n",
    ")\n",
    "\n",
    "geojson = json.loads(geojson)\n",
    "geojson['features'] = [feat for feat in geojson['features'] \n",
    "                       if feat['properties']['title'] != '0.00 ']\n",
    " \n",
    "# Set up the folium plot\n",
    "m = folium.Map( \n",
    "    zoom_start=1, \n",
    "    tiles=\"cartodbpositron\",\n",
    ")\n",
    "\n",
    "# Plot the contour plot on folium\n",
    "folium.GeoJson(\n",
    "    geojson,\n",
    "    style_function=lambda x: {\n",
    "        'color':     x['properties']['stroke'],\n",
    "        'weight':    0.5,\n",
    "        'opacity':   1,\n",
    "        'fillColor': x['properties']['stroke'],\n",
    "        'fillOpacity': float(x['properties']['title'])*15/100 + 0.1,\n",
    "    },\n",
    "    tooltip = folium.GeoJsonTooltip(\n",
    "        fields=['title'],\n",
    "        aliases=['% Likelihood']\n",
    "    ),\n",
    ").add_to(m)\n",
    "\n",
    "# add faults\n",
    "folium.GeoJson(\n",
    "    {\n",
    "        'type': 'FeatureCollection',\n",
    "        'features': fault_features,\n",
    "    },\n",
    "    style_function = lambda x: {\n",
    "        'color': 'grey',\n",
    "        'weight': 0.5,\n",
    "    }\n",
    ").add_to(m)\n",
    "\n",
    "# Add the colormap\n",
    "cm.caption = 'Percent Likelihood of Mag 5.5+ in any 7 Day Period'\n",
    "m.add_child(\n",
    "    cm,\n",
    ")\n",
    "\n",
    "# Fullscreen mode\n",
    "plugins.Fullscreen(\n",
    "    position='topright', \n",
    "    force_separate_button=True\n",
    ").add_to(m)\n",
    "\n",
    "# Plot the data\n",
    "m.save('../ds-application/app/static/likelihoods.html')\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter to clusters with >= 2000 observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clust_cnts = df.groupby(['clust_id']).size()\n",
    "clust_cnts = clust_cnts.loc[clust_cnts >= 2000]\n",
    "\n",
    "clust_df = clust_df.loc[clust_df['clust_id'].isin(clust_cnts.index)]\n",
    "df = df.loc[df['clust_id'].isin(clust_cnts.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clust_cnts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map Cluster Sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "import folium.plugins\n",
    "\n",
    "m = folium.Map(tiles='CartoDBpositron')\n",
    "clust_cnts = df.groupby(['clust_id', 'clust_lat', 'clust_lon']).size()\n",
    "\n",
    "for i, v in clust_cnts.items():\n",
    "    folium.CircleMarker(\n",
    "        i[1:], \n",
    "        radius=np.sqrt((v/100) / np.pi) * 2,\n",
    "        fill=True,\n",
    "        fill_opacity=0.5,\n",
    "        stroke=0,\n",
    "        popup=f\"id:{i[0]}<br>events: {v}\",\n",
    "    ).add_to(m)\n",
    "\n",
    "folium.plugins.Fullscreen(\n",
    "    position='topright',\n",
    "    force_separate_button=True,\n",
    ").add_to(m)\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify Cut-Off Magnitude for Each Node, `M_c`\n",
    "\n",
    "Here I compare OLS and Mode methods. For a more robust method, see [Bayesian method](https://medium.com/the-history-risk-forecast-of-perils/exploring-the-fascinating-world-of-incomplete-seismicity-data-part-i-ii-bayesian-inference-386338b43b71)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_GR_OLS(node_df):\n",
    "    n_steps = int((node_df['mag'].max() - node_df['mag'].min()) * 10) + 1\n",
    "    mag_range = np.linspace(\n",
    "        node_df['mag'].min(), \n",
    "        node_df['mag'].max(), \n",
    "        n_steps)\n",
    "    mag_range = [round(x, 2) for x in mag_range]\n",
    "\n",
    "    counts = pd.Series(\n",
    "        index=mag_range,\n",
    "        data=[sum(node_df['mag'] >= x) for x in mag_range]\n",
    "    )\n",
    "\n",
    "    X = np.array(counts.index).reshape(-1, 1)\n",
    "    y = np.log10(counts.values)\n",
    "    reg = LinearRegression().fit(X, y)\n",
    "    a = reg.intercept_\n",
    "    b = -1 * reg.coef_[0]\n",
    "    mse = mean_squared_error(y, reg.predict(X))\n",
    "    \n",
    "    return a, b, mse\n",
    "\n",
    "\n",
    "def mode_m_c_a_b(node_df):\n",
    "    node_df = node_df.copy()\n",
    "    node_df['mag'] = node_df['mag'].round(1)\n",
    "    node_df = node_df.loc[node_df['mag']<6]\n",
    "    m_c = node_df['mag'].mode()[0]\n",
    "    node_df = node_df.loc[node_df['mag']>=m_c]\n",
    "    a, b, _ = fit_GR_OLS(node_df)\n",
    "    \n",
    "    return m_c, a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clust_id = 609\n",
    "node_df = df.loc[df['clust_id']==clust_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# OLS method\n",
    "m_c, a, b = OLS_mc_a_b(node_df)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "node_df['mag'].hist(bins=50, ax=ax)\n",
    "plt.axvline(x=m_c, linewidth=2, color='r');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mode method\n",
    "m_c, a, b = mode_m_c_a_b(node_df)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "node_df['mag'].hist(bins=50, ax=ax)\n",
    "plt.axvline(x=m_c, linewidth=2, color='r');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply mode method\n",
    "clust_df['m_c'], clust_df['a'], clust_df['b'] = np.nan, np.nan, np.nan\n",
    "\n",
    "for clust_id in df['clust_id'].unique():\n",
    "    node_df = df.loc[df['clust_id']==clust_id]\n",
    "    m_c, a, b = mode_m_c_a_b(node_df)\n",
    "    clust_df.loc[clust_df['clust_id']==clust_id, ['m_c', 'a', 'b']] = m_c, a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clust_df['m_c'].hist();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Earthquake events by `m_c`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(clust_df[['clust_id', 'm_c', 'a', 'b']], on='clust_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.query(\"mag >= m_c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timeseries Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How Many Samples?\n",
    "\n",
    "In literature, 50 of the most recent quakes were analyzed when the magnitude cutoff was 4.0. However, some regions have a lower threshold, allowing for exponentially more events. Therefore, I will take exponentially more events when calculated features for those regions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_samples(m_c):\n",
    "    # From research:\n",
    "    #  m_c=4, y=50\n",
    "    # Extrapolated to the mc=0 case, with exponential increase\n",
    "    #  m_c=0, y=500000\n",
    "    # log10(n_events) = slope * m_c + y_int\n",
    "    y_int = np.log10(500000)\n",
    "    slope = -1 # by definition of logarithm\n",
    "\n",
    "    return int(round(10 ** (slope * m_c + y_int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each node, at weekly interval, collect past n events, predict whether 5.5+ will occur in the next 30 days\n",
    "min_date = df['time'].min().round('D')\n",
    "max_date = df['time'].max().round('D')\n",
    "strt_sunday = min_date - pd.to_timedelta(min_date.dayofweek, unit='d')\n",
    "end_sunday = (max_date - pd.to_timedelta(max_date.dayofweek, unit='d') - \n",
    "              pd.to_timedelta(30, unit='d')) # leave 30 day buffer at end to assess whether quake occured\n",
    "sampling_periods = pd.date_range(strt_sunday, end_sunday, freq='W')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_df = pd.DataFrame(columns=['clust_id','sample_date',\n",
    "                             'T_days','M_mean','dEsq',\n",
    "                             'a_lsq','b_lsq','a_mlk', 'b_mlk','mse',\n",
    "                             'diff_M_max_obs_exp','mu','c',\n",
    "                             'quake_next30'])\n",
    "\n",
    "for node_id, node_alltime_df in df.loc[df['clust_id'].isin([609, 554])].groupby(['clust_id']):\n",
    "    print(node_id)\n",
    "    \n",
    "    N_events = get_n_samples(node_alltime_df['mag'].min())\n",
    "    \n",
    "    for sample_date in sampling_periods:\n",
    "        # check if 5.5+ mag quake happend in next 30 days\n",
    "        condition = ((sample_date < node_alltime_df['time']) & \n",
    "                     (node_alltime_df['time'] < (sample_date + pd.to_timedelta(30, unit='d'))))\n",
    "        quake_next30 = sum(node_alltime_df.loc[condition, 'mag'] >= 5.5) > 0\n",
    "        \n",
    "        # take last 100 events, compute features\n",
    "        node_df = node_alltime_df.loc[node_alltime_df['time']<=sample_date]\n",
    "        \n",
    "        if len(node_df) < N_events:\n",
    "            continue\n",
    "        \n",
    "        node_df = node_df[-N_events:]\n",
    "\n",
    "        # T: time period in days\n",
    "        T = (node_df['time'].max() - node_df['time'].min()).days\n",
    "\n",
    "        # M_mean: mean Magnitude\n",
    "        M_mean = node_df['mag'].mean()\n",
    "\n",
    "        # dEsq: seismic energy release\n",
    "        dEsq = np.sum(\n",
    "            np.sqrt(\n",
    "                np.power(\n",
    "                    np.array([10]*len(node_df)), \n",
    "                    11.8+1.5*node_df['mag']\n",
    "                )\n",
    "            )\n",
    "        ) / T\n",
    "\n",
    "        # a and b values (2 methods) and MSE from GR law\n",
    "        n = len(node_df)\n",
    "        # some issues in here:\n",
    "        a_lsq, b_lsq, mse = fit_GR_OLS(node_df)\n",
    "        \n",
    "        b_mlk = np.log10(np.e) / (node_df['mag'].mean() - node_df['mag'].min())\n",
    "        a_mlk = np.log10(n) + b_mlk * node_df['mag'].min()\n",
    "\n",
    "        # difference between the maximum observed and the maximum expected\n",
    "        diff_M_max_obs_exp = node_df['mag'].max() - a_lsq / b_lsq\n",
    "        \n",
    "        # mean and stdev time between mag 4.5 & 5 events\n",
    "        diffs = node_df.loc[(4.5 <= node_df['mag']) & (node_df['mag'] <= 5), 'time'].diff()\n",
    "\n",
    "        mu = diffs.mean().total_seconds()\n",
    "        c = diffs.std().total_seconds() / mu\n",
    "  \n",
    "        # Maximum magnitude in last seven days\n",
    "        # date_7days = node_df['time'].max() - pd.to_timedelta(7, unit='d')\n",
    "        # x_6i = node_df.loc[node_df['time'] > date_7days, 'mag'].max()\n",
    "\n",
    "        x_df = x_df.append(pd.DataFrame({\n",
    "            'clust_id': [node_id],\n",
    "            'sample_date': [sample_date],\n",
    "            'T_days': [T],\n",
    "            'M_mean': [M_mean],\n",
    "            'dEsq': [dEsq],\n",
    "            'a_lsq': [a_lsq],\n",
    "            'b_lsq': [b_lsq],\n",
    "            'a_mlk': [a_mlk],\n",
    "            'b_mlk': [b_mlk],\n",
    "            'mse':[mse],\n",
    "            'diff_M_max_obs_exp': [diff_M_max_obs_exp],\n",
    "            'mu': [mu],\n",
    "            'c': [c],\n",
    "            'quake_next30': [quake_next30],\n",
    "        }))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x_df.groupby(['clust_id']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x_df.groupby(['clust_id'])['quake_next30'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_df.loc[(x_df['clust_id']==554) & \n",
    "         (x_df['quake_next30']==True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bad temporal patterns 598, 591, 557, 543, 536, 502, 448"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['clust_id']==598, 'time'].dt.year.hist(bins=20);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
